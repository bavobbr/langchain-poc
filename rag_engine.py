"""Core RAG engine for FIH Rules.

Provides ingestion (chunking + embeddings + persistence) and query handling
(contextualization, routing, retrieval, and synthesis).
"""


from langchain_core.documents import Document
import config
from database import PostgresVectorDB
from logger import get_logger

logger = get_logger(__name__)

class FIHRulesEngine:
    """High-level interface to embeddings, LLM, and vector DB."""

    def __init__(self):
        # Defer import of heavy Vertex AI libraries to instantiation time (reduces startup latency)
        from langchain_google_vertexai import VertexAIEmbeddings, VertexAI
        
        # Models
        self.embeddings = VertexAIEmbeddings(
            model_name=config.EMBEDDING_MODEL, 
            project=config.PROJECT_ID, 
            location=config.REGION
        )
        self.llm = VertexAI(
            model_name=config.LLM_MODEL,
            project=config.PROJECT_ID,
            location=config.REGION,
            temperature=0
        )
        # Database
        self.db = PostgresVectorDB()

    # Ingestion
    def ingest_pdf(self, file_path, variant, original_filename=None):
        """Parse a PDF, chunk, embed and persist under a ruleset variant.

        Validates 'variant' against config.VARIANTS to prevent unauthorized data creation.
        """
        # 0. Validate Input
        if variant not in config.VARIANTS:
            raise ValueError(f"Invalid variant '{variant}'. Allowed: {list(config.VARIANTS.keys())}")

        # 1. Ensure Schema
        self.db.ensure_schema()

        # 1. Protection: Check if variant already has data
        if self.db.variant_exists(variant):
            logger.warning(f"Protection: Data for variant '{variant}' already exists. Refusing to overwrite.")
            return -1

        # 2. Ingest
        # Dynamically load the configured loader (Online vs Batch) 
        # Lazy import loaders
        import loaders
        docai_loader = loaders.get_document_ai_loader()
        docs = docai_loader.load_and_chunk(file_path, variant, original_filename=original_filename)

        if not docs:
            logger.warning("No chunks generated!")
            return 0
        
        # Deduplication: Clear existing data for this variant
        logger.info(f"Cleaning existing '{variant}' data...")
        self.db.delete_variant(variant)
        
        # Embed
        logger.info(f"Generating embeddings for {len(docs)} chunks...")
        texts = [d.page_content for d in docs]
        metadatas = [d.metadata for d in docs]
        vectors = self.embeddings.embed_documents(texts)
        
        # Persist
        self.db.insert_batch(texts, vectors, variant, metadatas=metadatas)
        logger.info(f"Persisted {len(docs)} chunks to DB.")
        
        return len(docs)

    # Querying
    def query(self, user_input, history=[]):
        """Answer a user question using contextualization, routing, and RAG."""
        # Reformulate & route
        standalone_query = self._contextualize_query(history, user_input)
        
        detected_variant = self._route_query(standalone_query)
        if detected_variant not in config.VARIANTS: 
            detected_variant = "outdoor"
        # Embed query
        query_vector = self.embeddings.embed_query(standalone_query)
        # Retrieve
        results = self.db.search(query_vector, detected_variant, k=config.RETRIEVAL_K)
        
        # Convert DB results back to LangChain Documents for consistency
        docs = [Document(page_content=r["content"], metadata=r["metadata"]) for r in results]
        
        # Synthesize answer
        context_pieces = []
        for d in docs:
            meta = d.metadata
            # Fallback values if metadata is empty/legacy
            heading = meta.get("heading", "Reference")
            chapter = meta.get("chapter", "")
            section = meta.get("section", "")
            
            # Construct Citation Header
            # e.g. [Rule 9.12] [Source: rules.pdf p.42] (Context: PLAYING THE GAME > Field of Play)
            source_file = meta.get("source_file", "unknown")
            page_num = meta.get("page", "?")
            
            context_string = f"[{heading}] [Source: {source_file} p.{page_num}]"
            if chapter or section:
                context_string += f" (Context: {chapter} > {section})"
            
            context_pieces.append(f"{context_string}\n{d.page_content}")

        context_text = "\n\n".join(context_pieces)
        
        if not context_text:
            return {
                "answer": f"I checked the **{detected_variant}** rules but couldn't find an answer.",
                "standalone_query": standalone_query,
                "variant": detected_variant,
                "source_docs": []
            }

        full_prompt = f"""
        Expert FIH Umpire for {detected_variant.upper()}. 
        Answer based on Context. Cite Rules.
        
        CONTEXT:
        {context_text}
        
        QUESTION:
        {standalone_query}
        """
        answer = self.llm.invoke(full_prompt)
        
        return {
            "answer": answer,
            "standalone_query": standalone_query,
            "variant": detected_variant,
            "source_docs": docs
        }

    def _contextualize_query(self, history, query):
        """Rewrite the latest user message as a standalone query."""
        if not history: return query
        history_str = "\n".join([f"{role}: {txt}" for role, txt in history[-4:]])
        prompt = f"""Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.
        
        Do NOT answer the question. Just rewrite it to be self-contained. Start the question with whether the question is about outdoor, indoor or hockey5s variant.
        If not clear from context, default to outdoor.
        
        Chat History:
        {history_str}
        
        Follow Up Input: {query}
        
        Standalone Question:"""
        return self.llm.invoke(prompt).strip()

    def _route_query(self, query):
        """Return 'outdoor' | 'indoor' | 'hockey5s' based on content."""
        prompt = f"Analyze Field Hockey question. Return 'outdoor', 'indoor', or 'hockey5s'. Default to 'outdoor'.\nQUESTION: {query}"
        return self.llm.invoke(prompt).strip().lower().replace("'", "").replace('"', "")
